{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ea7f19-8233-44c8-95c1-3c51747a7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/jovyan/BCICIV_2a_gdf/A02T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG-Fz, EEG, EEG, EEG, EEG, EEG, EEG, EEG-C3, EEG, EEG-Cz, EEG, EEG-C4, EEG, EEG, EEG, EEG, EEG, EEG, EEG, EEG-Pz, EEG, EEG, EOG-left, EOG-central, EOG-right\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 677168  =      0.000 ...  2708.672 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['769', '770', '771', '772']\n",
      "Not setting metadata\n",
      "288 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 288 events and 1126 original time points ...\n",
      "1 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_367282/1806478324.py:26: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  data = epochs.get_data()  # shape is (n_epochs, n_channels, n_times)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 287 epochs and their features to /home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2\n",
      "Saved epoch plots:\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2/epoch_1_class_0.npy_without_axes.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2/epoch_62_class_3.npy_without_axes.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2/epoch_72_class_2.npy_without_axes.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2/epoch_196_class_2.npy_without_axes.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2/epoch_145_class_1.npy_without_axes.png\n",
      "Class distribution plot saved at: /home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2/class_distribution.png\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path to the dataset and the output directory\n",
    "gdf_path = '/home/jovyan/BCICIV_2a_gdf/A02T.gdf'\n",
    "output_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 1: Load the BCI-IV 2a dataset\n",
    "raw = mne.io.read_raw_gdf(gdf_path, preload=True)\n",
    "\n",
    "# Remove the last three channels (EOG-left, EOG-central, and EOG-right)\n",
    "raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])\n",
    "\n",
    "# Step 2: Extract epochs for each class using provided event IDs\n",
    "event_id = {'769': 7, '770': 8, '771': 9, '772': 10}\n",
    "events, _ = mne.events_from_annotations(raw, event_id=event_id)\n",
    "tmin, tmax = 1.5, 6  # 4.5 seconds epochs starting at 1.5s\n",
    "\n",
    "# Creating epochs for each class\n",
    "epochs = mne.Epochs(raw, events, event_id, tmin, tmax, baseline=None, preload=True)\n",
    "data = epochs.get_data()  # shape is (n_epochs, n_channels, n_times)\n",
    "\n",
    "# Standardize each channel\n",
    "data = (data - data.mean(axis=2, keepdims=True)) / data.std(axis=2, keepdims=True)\n",
    "\n",
    "# Initial Temporal Feature Extraction Module (iTFE Module)\n",
    "def extract_temporal_features(epoch_data):\n",
    "    # Extract temporal features: mean and std for each channel\n",
    "    mean_features = epoch_data.mean(axis=1)\n",
    "    std_features = epoch_data.std(axis=1)\n",
    "    # Concatenate features to form a feature vector\n",
    "    features = np.concatenate((mean_features, std_features), axis=0)\n",
    "    return features\n",
    "\n",
    "# Step 3: Save each epoch and its features\n",
    "class_counts = {7: 0, 8: 0, 9: 0, 10: 0}\n",
    "\n",
    "for i, epoch_data in enumerate(data):\n",
    "    # Adjust labels to start from 0 by subtracting 7 from each label\n",
    "    label = epochs.events[i, -1]\n",
    "    file_path = os.path.join(output_dir, f'epoch_{i+1}_class_{label - 7}.npy')\n",
    "    np.save(file_path, epoch_data)\n",
    "    \n",
    "    # Extract and save features\n",
    "    features = extract_temporal_features(epoch_data)\n",
    "    features_path = os.path.join(output_dir, f'features_{i+1}_class_{label - 7}.npy')\n",
    "    np.save(features_path, features)\n",
    "    \n",
    "    class_counts[label] += 1\n",
    "\n",
    "print(f'Saved {len(data)} epochs and their features to {output_dir}')\n",
    "\n",
    "# Step 4: Plot and save some epochs without axes and color bar\n",
    "def plot_and_save_epochs(output_dir, save_dir, num_files_to_plot=5):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_files = [f for f in os.listdir(output_dir) if f.endswith('.npy') and 'epoch' in f]\n",
    "    saved_file_paths = []\n",
    "    for i in range(min(num_files_to_plot, len(saved_files))):\n",
    "        file_path = os.path.join(output_dir, saved_files[i])\n",
    "        epoch_data = np.load(file_path)\n",
    "        \n",
    "        # Plot without axes and color bar\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(epoch_data, aspect='auto', cmap='jet')\n",
    "        plt.axis('off')\n",
    "        save_path = os.path.join(save_dir, f'{saved_files[i]}_without_axes.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        saved_file_paths.append(save_path)\n",
    "    \n",
    "    return saved_file_paths\n",
    "\n",
    "# Step 5: Plot the distribution of files per class and save image\n",
    "def plot_and_save_class_distribution(class_counts, save_path):\n",
    "    labels = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts, color='blue')\n",
    "    plt.xlabel('Class Labels')\n",
    "    plt.ylabel('Number of Files')\n",
    "    plt.title('Distribution of Files per Class')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    return save_path\n",
    "\n",
    "# Directory to save plots\n",
    "plot_save_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2_p2'\n",
    "os.makedirs(plot_save_dir, exist_ok=True)\n",
    "\n",
    "# Plot and save example epochs\n",
    "saved_epoch_files = plot_and_save_epochs(output_dir, plot_save_dir)\n",
    "\n",
    "# Save the distribution of files per class\n",
    "class_distribution_path = plot_and_save_class_distribution(class_counts, os.path.join(plot_save_dir, 'class_distribution.png'))\n",
    "\n",
    "# Print the paths to the saved files\n",
    "print(\"Saved epoch plots:\")\n",
    "for file_path in saved_epoch_files:\n",
    "    print(file_path)\n",
    "\n",
    "print(f\"Class distribution plot saved at: {class_distribution_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51c7af7f-133c-45e1-9f5f-01e79d2d66b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/jovyan/BCICIV_2a_gdf/A02T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG-Fz, EEG, EEG, EEG, EEG, EEG, EEG, EEG-C3, EEG, EEG-Cz, EEG, EEG-C4, EEG, EEG, EEG, EEG, EEG, EEG, EEG, EEG-Pz, EEG, EEG, EOG-left, EOG-central, EOG-right\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 677168  =      0.000 ...  2708.672 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['769', '770', '771', '772']\n",
      "Not setting metadata\n",
      "288 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 288 events and 1126 original time points ...\n",
      "1 bad epochs dropped\n",
      "Feature matrix shape: (287, 25344)\n",
      "Predictions: [0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0 1 1 1 0 0 0 0 1\n",
      " 1 1 0 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 1 1 1 1 1 1 0 0 1 0 0 0 0 0 1 1 1 1 1\n",
      " 0 0 0 1 1 0 1 1 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 1 0 0\n",
      " 1 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0\n",
      " 1 1 1 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1\n",
      " 1 0 1 0 1 1 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 0\n",
      " 1 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 1 1 0 0]\n",
      "Saved epoch plots:\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_1_class_0.npy_without_axes.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_62_class_3.npy_without_axes.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_72_class_2.npy_without_axes.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_196_class_2.npy_without_axes.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_145_class_1.npy_without_axes.png\n",
      "Class distribution plot saved at: /home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/class_distribution.png\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "from scipy.ndimage import convolve1d\n",
    "\n",
    "# Define the path to the dataset and the output directory\n",
    "gdf_path = '/home/jovyan/BCICIV_2a_gdf/A02T.gdf'\n",
    "output_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 1: Load the BCI-IV 2a dataset\n",
    "raw = mne.io.read_raw_gdf(gdf_path, preload=True)\n",
    "\n",
    "# Remove the last three channels (EOG-left, EOG-central, and EOG-right)\n",
    "raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])\n",
    "\n",
    "# Step 2: Extract epochs for each class using provided event IDs\n",
    "event_id = {'769': 7, '770': 8, '771': 9, '772': 10}\n",
    "events, _ = mne.events_from_annotations(raw, event_id=event_id)\n",
    "tmin, tmax = 1.5, 6  # 4.5 seconds epochs starting at 1.5s\n",
    "\n",
    "# Creating epochs for each class\n",
    "epochs = mne.Epochs(raw, events, event_id, tmin, tmax, baseline=None, preload=True)\n",
    "data = epochs.get_data(copy=True)  # shape is (n_epochs, n_channels, n_times)\n",
    "\n",
    "# Standardize each channel\n",
    "data = (data - data.mean(axis=2, keepdims=True)) / data.std(axis=2, keepdims=True)\n",
    "\n",
    "# Initial Temporal Feature Extraction (iTFE) Module with Convolution\n",
    "def apply_convolution(epoch_data, kernel):\n",
    "    return convolve1d(epoch_data, kernel, axis=-1, mode='constant')\n",
    "\n",
    "def extract_temporal_features_with_convolution(epoch_data):\n",
    "    kernels = [np.ones((3,))/3, np.ones((5,))/5, np.ones((11,))/11]\n",
    "    mean_features = []\n",
    "    std_features = []\n",
    "    for kernel in kernels:\n",
    "        convolved_data = apply_convolution(epoch_data, kernel)\n",
    "        mean_features.append(convolved_data.mean(axis=1))\n",
    "        std_features.append(convolved_data.std(axis=1))\n",
    "    mean_features = np.mean(mean_features, axis=0)\n",
    "    std_features = np.mean(std_features, axis=0)\n",
    "    return np.concatenate((mean_features, std_features), axis=0)\n",
    "\n",
    "# Deep EEG-Channel-attention (DEC) Module\n",
    "def eeg_channel_attention(features, channel_weights):\n",
    "    # Assuming channel_weights is the same length as number of channels\n",
    "    n_channels = len(channel_weights)\n",
    "    mean_features = features[:n_channels]\n",
    "    std_features = features[n_channels:]\n",
    "    \n",
    "    # Apply channel weights\n",
    "    weighted_mean_features = mean_features * channel_weights\n",
    "    weighted_std_features = std_features * channel_weights\n",
    "    \n",
    "    # Combine weighted features\n",
    "    weighted_features = np.concatenate((weighted_mean_features, weighted_std_features), axis=0)\n",
    "    return weighted_features\n",
    "\n",
    "# Wavelet-based Temporal-Spectral-attention (WTS) Module\n",
    "def wavelet_temporal_spectral_attention(epoch_data, wavelet='db4', level=4):\n",
    "    coeffs = pywt.wavedec(epoch_data, wavelet, level=level)\n",
    "    return np.concatenate(coeffs, axis=1)\n",
    "\n",
    "# Simple Discrimination Module\n",
    "def simple_discrimination(feature_matrix):\n",
    "    # Placeholder for a simple classification model\n",
    "    # Assuming a binary classification for simplicity\n",
    "    return np.random.randint(0, 2, size=feature_matrix.shape[0])\n",
    "\n",
    "# Initialize channel weights (assuming equal importance initially)\n",
    "channel_weights = np.ones(data.shape[1])\n",
    "\n",
    "# Step 3: Process each epoch through the modules\n",
    "class_counts = {7: 0, 8: 0, 9: 0, 10: 0}\n",
    "features_list = []\n",
    "\n",
    "for i, epoch_data in enumerate(data):\n",
    "    # Extract temporal features using the iTFE module\n",
    "    temporal_features = extract_temporal_features_with_convolution(epoch_data)\n",
    "    \n",
    "    # Deep EEG-Channel-attention\n",
    "    attended_features = eeg_channel_attention(temporal_features, channel_weights)\n",
    "    \n",
    "    # Wavelet-based Temporal-Spectral-attention\n",
    "    wts_features = wavelet_temporal_spectral_attention(epoch_data)\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = np.concatenate((attended_features, wts_features.flatten()), axis=0)\n",
    "    features_list.append(combined_features)\n",
    "    \n",
    "    # Adjust labels to start from 0 by subtracting 7 from each label\n",
    "    label = epochs.events[i, -1]\n",
    "    class_counts[label] += 1\n",
    "\n",
    "# Convert features list to a matrix\n",
    "feature_matrix = np.array(features_list)\n",
    "\n",
    "# Simple Discrimination\n",
    "predictions = simple_discrimination(feature_matrix)\n",
    "\n",
    "# Print the results\n",
    "print(f'Feature matrix shape: {feature_matrix.shape}')\n",
    "print(f'Predictions: {predictions}')\n",
    "\n",
    "# Save feature matrix and predictions\n",
    "np.save(os.path.join(output_dir, 'feature_matrix.npy'), feature_matrix)\n",
    "np.save(os.path.join(output_dir, 'predictions.npy'), predictions)\n",
    "\n",
    "# Step 4: Plot and save some epochs without axes and color bar\n",
    "def plot_and_save_epochs(output_dir, save_dir, num_files_to_plot=5):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_files = [f for f in os.listdir(output_dir) if f.endswith('.npy') and 'epoch' in f]\n",
    "    saved_file_paths = []\n",
    "    for i in range(min(num_files_to_plot, len(saved_files))):\n",
    "        file_path = os.path.join(output_dir, saved_files[i])\n",
    "        epoch_data = np.load(file_path)\n",
    "        \n",
    "        # Plot without axes and color bar\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(epoch_data, aspect='auto', cmap='jet')\n",
    "        plt.axis('off')\n",
    "        save_path = os.path.join(save_dir, f'{saved_files[i]}_without_axes.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        saved_file_paths.append(save_path)\n",
    "    \n",
    "    return saved_file_paths\n",
    "\n",
    "# Step 5: Plot the distribution of files per class and save image\n",
    "def plot_and_save_class_distribution(class_counts, save_path):\n",
    "    labels = list(class_counts.keys())\n",
    "    counts = list(class_counts.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(labels, counts, color='blue')\n",
    "    plt.xlabel('Class Labels')\n",
    "    plt.ylabel('Number of Files')\n",
    "    plt.title('Distribution of Files per Class')\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    return save_path\n",
    "\n",
    "# Directory to save plots\n",
    "plot_save_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5'\n",
    "os.makedirs(plot_save_dir, exist_ok=True)\n",
    "\n",
    "# Plot and save example epochs\n",
    "saved_epoch_files = plot_and_save_epochs(output_dir, plot_save_dir)\n",
    "\n",
    "# Save the distribution of files per class\n",
    "class_distribution_path = plot_and_save_class_distribution(class_counts, os.path.join(plot_save_dir, 'class_distribution.png'))\n",
    "\n",
    "# Print the paths to the saved files\n",
    "print(\"Saved epoch plots:\")\n",
    "for file_path in saved_epoch_files:\n",
    "    print(file_path)\n",
    "\n",
    "print(f\"Class distribution plot saved at: {class_distribution_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d60b98-5fa8-481f-86fb-300cba57a734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/jovyan/BCICIV_2a_gdf/A02T.gdf...\n",
      "GDF file detected\n",
      "Setting channel info structure...\n",
      "Could not determine channel type of the following channels, they will be set as EEG:\n",
      "EEG-Fz, EEG, EEG, EEG, EEG, EEG, EEG, EEG-C3, EEG, EEG-Cz, EEG, EEG-C4, EEG, EEG, EEG, EEG, EEG, EEG, EEG, EEG-Pz, EEG, EEG, EOG-left, EOG-central, EOG-right\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 677168  =      0.000 ...  2708.672 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/contextlib.py:144: RuntimeWarning: Channel names are not unique, found duplicates for: {'EEG'}. Applying running numbers for duplicates.\n",
      "  next(self.gen)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['769', '770', '771', '772']\n",
      "Not setting metadata\n",
      "288 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 288 events and 1126 original time points ...\n",
      "1 bad epochs dropped\n",
      "Feature matrix shape: (287, 132)\n",
      "Saved epoch feature plots after iTFE:\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_1_iTFE_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_2_iTFE_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_3_iTFE_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_4_iTFE_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_5_iTFE_features.png\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import convolve1d\n",
    "\n",
    "# Define the path to the dataset and the output directory\n",
    "gdf_path = '/home/jovyan/BCICIV_2a_gdf/A02T.gdf'\n",
    "output_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 1: Load the BCI-IV 2a dataset\n",
    "raw = mne.io.read_raw_gdf(gdf_path, preload=True)\n",
    "\n",
    "# Remove the last three channels (EOG-left, EOG-central, and EOG-right)\n",
    "raw.drop_channels(['EOG-left', 'EOG-central', 'EOG-right'])\n",
    "\n",
    "# Step 2: Extract epochs for each class using provided event IDs\n",
    "event_id = {'769': 7, '770': 8, '771': 9, '772': 10}\n",
    "events, _ = mne.events_from_annotations(raw, event_id=event_id)\n",
    "tmin, tmax = 1.5, 6  # 4.5 seconds epochs starting at 1.5s\n",
    "\n",
    "# Creating epochs for each class\n",
    "epochs = mne.Epochs(raw, events, event_id, tmin, tmax, baseline=None, preload=True)\n",
    "data = epochs.get_data(copy=True)  # shape is (n_epochs, n_channels, n_times)\n",
    "\n",
    "# Standardize each channel\n",
    "data = (data - data.mean(axis=2, keepdims=True)) / data.std(axis=2, keepdims=True)\n",
    "\n",
    "# Step 3: Apply the iTFE module\n",
    "def apply_convolution(epoch_data, kernel):\n",
    "    return convolve1d(epoch_data, kernel, axis=-1, mode='constant')\n",
    "\n",
    "def extract_temporal_features_with_convolution(epoch_data):\n",
    "    kernels = [np.ones((3,))/3, np.ones((5,))/5, np.ones((11,))/11]\n",
    "    features = []\n",
    "    for kernel in kernels:\n",
    "        convolved_data = apply_convolution(epoch_data, kernel)\n",
    "        mean_features = convolved_data.mean(axis=1)\n",
    "        std_features = convolved_data.std(axis=1)\n",
    "        features.append(mean_features)\n",
    "        features.append(std_features)\n",
    "    # Concatenate the features from different kernels\n",
    "    return np.concatenate(features, axis=0)\n",
    "\n",
    "# Initialize an empty list to store features\n",
    "features_list = []\n",
    "\n",
    "# Process each epoch through the iTFE module\n",
    "for i, epoch_data in enumerate(data):\n",
    "    temporal_features = extract_temporal_features_with_convolution(epoch_data)\n",
    "    features_list.append(temporal_features)\n",
    "\n",
    "# Convert features list to a matrix\n",
    "feature_matrix = np.array(features_list)\n",
    "\n",
    "# Print the shape of the feature matrix\n",
    "print(f'Feature matrix shape: {feature_matrix.shape}')\n",
    "\n",
    "# Save the feature matrix\n",
    "np.save(os.path.join(output_dir, 'feature_matrix_iTFE.npy'), feature_matrix)\n",
    "\n",
    "# Plot and save some example epochs after iTFE processing\n",
    "def plot_and_save_example_epochs(feature_matrix, save_dir, num_files_to_plot=5):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_file_paths = []\n",
    "    for i in range(min(num_files_to_plot, len(feature_matrix))):\n",
    "        epoch_features = feature_matrix[i]\n",
    "        \n",
    "        # Plot the features\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(epoch_features)\n",
    "        plt.title(f'Epoch {i+1} Features after iTFE')\n",
    "        save_path = os.path.join(save_dir, f'epoch_{i+1}_iTFE_features.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        saved_file_paths.append(save_path)\n",
    "    \n",
    "    return saved_file_paths\n",
    "\n",
    "# Directory to save plots\n",
    "plot_save_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5'\n",
    "os.makedirs(plot_save_dir, exist_ok=True)\n",
    "\n",
    "# Plot and save example epochs\n",
    "saved_epoch_files = plot_and_save_example_epochs(feature_matrix, plot_save_dir)\n",
    "\n",
    "# Print the paths to the saved files\n",
    "print(\"Saved epoch feature plots after iTFE:\")\n",
    "for file_path in saved_epoch_files:\n",
    "    print(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f3d788-75d8-4b6e-af0b-02745f631433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded feature matrix shape: (287, 132)\n",
      "Saved epoch feature plots after DEC:\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_1_dec_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_2_dec_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_3_dec_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_4_dec_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_5_dec_features.png\n",
      "DEC feature matrix saved at: /home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2/dec_feature_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define paths\n",
    "feature_matrix_path = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2/feature_matrix_iTFE.npy'\n",
    "output_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2'\n",
    "plot_save_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5'\n",
    "\n",
    "# Load the feature matrix from the iTFE module\n",
    "feature_matrix = np.load(feature_matrix_path)\n",
    "\n",
    "# Print the shape of the loaded feature matrix\n",
    "print(f'Loaded feature matrix shape: {feature_matrix.shape}')\n",
    "\n",
    "# Helper functions for ELU and Softmax\n",
    "def elu(x, alpha=1.0):\n",
    "    return np.where(x >= 0, x, alpha * (np.exp(x) - 1))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "# Custom Dense layer implementation\n",
    "def dense_layer(input_data, weights, bias):\n",
    "    return np.dot(input_data, weights) + bias\n",
    "\n",
    "# Step 4: Apply the DEC module\n",
    "def dec_module(input_features):\n",
    "    # Squeeze operation\n",
    "    global_avg_pool = np.mean(input_features, axis=-1)\n",
    "    \n",
    "    # Define the weights and biases for the dense layers\n",
    "    input_dim = global_avg_pool.shape[-1]\n",
    "    hidden_units = input_dim // 2\n",
    "    \n",
    "    weights_1 = np.random.randn(input_dim, hidden_units) * 0.01\n",
    "    bias_1 = np.zeros(hidden_units)\n",
    "    weights_2 = np.random.randn(hidden_units, input_dim) * 0.01\n",
    "    bias_2 = np.zeros(input_dim)\n",
    "    \n",
    "    # Excitation operation\n",
    "    fc1_output = elu(dense_layer(global_avg_pool, weights_1, bias_1))\n",
    "    fc2_output = softmax(dense_layer(fc1_output, weights_2, bias_2))\n",
    "    \n",
    "    # Rescale features\n",
    "    rescaled_features = input_features * fc2_output[:, np.newaxis]\n",
    "    return rescaled_features\n",
    "\n",
    "# Apply DEC module to the feature matrix\n",
    "dec_feature_matrix = np.array([dec_module(features.reshape(22, -1)) for features in feature_matrix])\n",
    "\n",
    "# Save the DEC feature matrix\n",
    "dec_feature_matrix_path = os.path.join(output_dir, 'dec_feature_matrix.npy')\n",
    "np.save(dec_feature_matrix_path, dec_feature_matrix)\n",
    "\n",
    "# Plot and save some example epochs after DEC processing\n",
    "def plot_and_save_example_epochs(feature_matrix, save_dir, num_files_to_plot=5):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_file_paths = []\n",
    "    for i in range(min(num_files_to_plot, len(feature_matrix))):\n",
    "        epoch_features = feature_matrix[i]\n",
    "        \n",
    "        # Plot the features\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(epoch_features.flatten())\n",
    "        plt.title(f'Epoch {i+1} Features after DEC')\n",
    "        save_path = os.path.join(save_dir, f'epoch_{i+1}_dec_features.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        saved_file_paths.append(save_path)\n",
    "    \n",
    "    return saved_file_paths\n",
    "\n",
    "# Directory to save plots\n",
    "os.makedirs(plot_save_dir, exist_ok=True)\n",
    "\n",
    "# Plot and save example epochs\n",
    "saved_epoch_files = plot_and_save_example_epochs(dec_feature_matrix, plot_save_dir)\n",
    "\n",
    "# Print the paths to the saved files\n",
    "print(\"Saved epoch feature plots after DEC:\")\n",
    "for file_path in saved_epoch_files:\n",
    "    print(file_path)\n",
    "\n",
    "# Print path to the saved DEC feature matrix\n",
    "print(f\"DEC feature matrix saved at: {dec_feature_matrix_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c53033-47fd-4701-a32d-6142045eee6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DEC feature matrix shape: (287, 22, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_367282/1570220752.py:34: RuntimeWarning: invalid value encountered in divide\n",
      "  t_statistics = (mean1 - mean2) / np.sqrt((std1**2 / n1) + (std2**2 / n2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weighted CWT feature plots:\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_1_weighted_cwt_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_2_weighted_cwt_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_3_weighted_cwt_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_4_weighted_cwt_features.png\n",
      "/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5/epoch_5_weighted_cwt_features.png\n",
      "Weighted CWT feature matrix saved at: /home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2/weighted_cwt_feature_matrix.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "\n",
    "# Define paths\n",
    "dec_feature_matrix_path = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2/dec_feature_matrix.npy'\n",
    "output_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub2'\n",
    "plot_save_dir = '/home/jovyan/BCICIV_2a_gdf/processed_epoch_sub5'\n",
    "\n",
    "# Load the DEC feature matrix\n",
    "dec_feature_matrix = np.load(dec_feature_matrix_path)\n",
    "\n",
    "# Print the shape of the loaded feature matrix\n",
    "print(f'Loaded DEC feature matrix shape: {dec_feature_matrix.shape}')\n",
    "\n",
    "# Step 1: Apply Continuous Wavelet Transform (CWT) using Morlet wavelet\n",
    "def apply_cwt(epoch_data, scales=np.arange(1, 31), wavelet='cmor'):\n",
    "    coefficients, frequencies = pywt.cwt(epoch_data, scales, wavelet, axis=-1)\n",
    "    return coefficients\n",
    "\n",
    "# Step 2: Transform DEC features using CWT\n",
    "cwt_feature_matrix = np.array([apply_cwt(epoch) for epoch in dec_feature_matrix])\n",
    "\n",
    "# Step 3: Perform Independent Sample T-statistics\n",
    "def calculate_t_statistics(group1, group2):\n",
    "    mean1 = np.mean(group1, axis=0)\n",
    "    mean2 = np.mean(group2, axis=0)\n",
    "    std1 = np.std(group1, axis=0, ddof=1)\n",
    "    std2 = np.std(group2, axis=0, ddof=1)\n",
    "    n1 = group1.shape[0]\n",
    "    n2 = group2.shape[0]\n",
    "\n",
    "    t_statistics = (mean1 - mean2) / np.sqrt((std1**2 / n1) + (std2**2 / n2))\n",
    "    return t_statistics\n",
    "\n",
    "# Group labels based on MI tasks (assuming the classes are labeled from 0 to 3)\n",
    "# Adjust the indices according to your specific data\n",
    "group_A_indices = np.where((epochs.events[:, -1] == 0) | (epochs.events[:, -1] == 1))[0]\n",
    "group_B_indices = np.where((epochs.events[:, -1] == 2) | (epochs.events[:, -1] == 3))[0]\n",
    "\n",
    "group_A = cwt_feature_matrix[group_A_indices]\n",
    "group_B = cwt_feature_matrix[group_B_indices]\n",
    "\n",
    "# Calculate T-statistics between groups A and B\n",
    "t_statistics = calculate_t_statistics(group_A, group_B)\n",
    "\n",
    "# Step 4: Weight features using T-statistics\n",
    "weighted_cwt_feature_matrix = cwt_feature_matrix * t_statistics[np.newaxis, :, :, :]\n",
    "\n",
    "# Save the weighted CWT feature matrix\n",
    "weighted_cwt_feature_matrix_path = os.path.join(output_dir, 'weighted_cwt_feature_matrix.npy')\n",
    "np.save(weighted_cwt_feature_matrix_path, weighted_cwt_feature_matrix)\n",
    "\n",
    "# Step 5: Plot and save some example weighted CWT feature maps\n",
    "def plot_and_save_example_weighted_cwt_features(feature_matrix, save_dir, num_files_to_plot=5):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    saved_file_paths = []\n",
    "    \n",
    "    for i in range(min(num_files_to_plot, len(feature_matrix))):\n",
    "        epoch_features = feature_matrix[i]\n",
    "\n",
    "        # Calculate the magnitude of CWT coefficients\n",
    "        magnitude_features = np.abs(epoch_features)\n",
    "\n",
    "        # Normalize magnitude to [0, 1] for visualization\n",
    "        normalized_features = (magnitude_features - np.min(magnitude_features)) / (np.max(magnitude_features) - np.min(magnitude_features))\n",
    "\n",
    "        # Plot the features\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(normalized_features.mean(axis=0), aspect='auto', cmap='jet', extent=[0, normalized_features.shape[-1], normalized_features.shape[1], 1])\n",
    "        plt.title(f'Epoch {i+1} Weighted CWT Features')\n",
    "        plt.colorbar(label='Magnitude')\n",
    "        save_path = os.path.join(save_dir, f'epoch_{i+1}_weighted_cwt_features.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        saved_file_paths.append(save_path)\n",
    "\n",
    "    return saved_file_paths\n",
    "\n",
    "\n",
    "# Plot and save example weighted CWT feature maps\n",
    "saved_epoch_files = plot_and_save_example_weighted_cwt_features(weighted_cwt_feature_matrix, plot_save_dir)\n",
    "\n",
    "# Print the paths to the saved files\n",
    "print(\"Saved weighted CWT feature plots:\")\n",
    "for file_path in saved_epoch_files:\n",
    "    print(file_path)\n",
    "\n",
    "# Print path to the saved weighted CWT feature matrix\n",
    "print(f\"Weighted CWT feature matrix saved at: {weighted_cwt_feature_matrix_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "259795f7-841f-4e0d-8e9c-e8279d5d7726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File A01E.gdf not found in data/BCICIV_2a_gdf\n",
      "File A02E.gdf not found in data/BCICIV_2a_gdf\n",
      "File A03E.gdf not found in data/BCICIV_2a_gdf\n",
      "File A04E.gdf not found in data/BCICIV_2a_gdf\n",
      "File A05E.gdf not found in data/BCICIV_2a_gdf\n",
      "File A06E.gdf not found in data/BCICIV_2a_gdf\n",
      "File A07E.gdf not found in data/BCICIV_2a_gdf\n",
      "File A08E.gdf not found in data/BCICIV_2a_gdf\n",
      "File A09E.gdf not found in data/BCICIV_2a_gdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pywt\n",
    "import mne\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis, skew\n",
    "from spectrum import arburg\n",
    "from mne.decoding import CSP\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, dataset_path='data/BCICIV_2a_gdf', save_path='output/path', n_sub=9, sub_list=None,\n",
    "                 sample_freq=250, data_aug=False, n_hop=0.1, window_sz=2, low_freq=0.5, high_freq=35,\n",
    "                 wavelet=True, f_bank=False, wpd_noc=False, n_bands=8, low_frequencies=None,\n",
    "                 high_frequencies=None, feature_list=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.save_path = save_path\n",
    "        self.n_sub = n_sub\n",
    "        self.subjects = sub_list if sub_list is not None else list(range(1, n_sub + 1))\n",
    "        self.sample_freq = sample_freq\n",
    "        self.data_aug = data_aug\n",
    "        self.n_hop = n_hop\n",
    "        self.window_sz = window_sz\n",
    "        self.low_freq = low_freq\n",
    "        self.high_freq = high_freq\n",
    "        self.wavelet = wavelet\n",
    "        self.f_bank = f_bank\n",
    "        self.wpd_noc = wpd_noc\n",
    "        self.n_bands = n_bands\n",
    "        self.low_frequencies = low_frequencies if low_frequencies is not None else np.arange(4, 37, 1)\n",
    "        self.high_frequencies = high_frequencies if high_frequencies is not None else np.arange(8, 41, 1)\n",
    "        self.feature_list = feature_list if feature_list is not None else [0, 1, 2, 6, 8, 12, 13, 19, 21]\n",
    "\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "    def load_gdf_data(self, file_path):\n",
    "        raw = mne.io.read_raw_gdf(file_path, preload=True)\n",
    "        raw.filter(self.low_freq, self.high_freq, fir_design='firwin')\n",
    "        return raw\n",
    "\n",
    "    def extract_features(self, raw):\n",
    "        csp = CSP(n_components=2, reg=None, log=True, norm_trace=False)\n",
    "        epochs = mne.make_fixed_length_epochs(raw, duration=self.window_sz, overlap=self.n_hop)\n",
    "        X = epochs.get_data()\n",
    "        y = epochs.events[:, -1]\n",
    "\n",
    "        X = X.reshape(X.shape[0], -1)  # Flatten the data for CSP\n",
    "        X = csp.fit_transform(X, y)\n",
    "\n",
    "        features = []\n",
    "        for x in X:\n",
    "            feature_vector = []\n",
    "            if 0 in self.feature_list:  # Mean Absolute Value\n",
    "                feature_vector.append(np.mean(np.abs(x)))\n",
    "            if 1 in self.feature_list:  # Root Mean Square\n",
    "                feature_vector.append(np.sqrt(np.mean(x**2)))\n",
    "            if 2 in self.feature_list:  # Average amplitude change\n",
    "                feature_vector.append(np.mean(np.diff(x)))\n",
    "            if 6 in self.feature_list:  # Mean Energy\n",
    "                feature_vector.append(np.mean(x**2))\n",
    "            if 8 in self.feature_list:  # Standard deviation\n",
    "                feature_vector.append(np.std(x))\n",
    "            if 12 in self.feature_list:  # Energy_ratio\n",
    "                feature_vector.append(np.sum(x**2) / np.sum(x))\n",
    "            if 13 in self.feature_list:  # Hjorth Activity and Complexity\n",
    "                feature_vector.extend(self.hjorth(x))\n",
    "            if 19 in self.feature_list:  # FFT features\n",
    "                feature_vector.extend(self.fft_features(x))\n",
    "            if 21 in self.feature_list:  # Autoregression model- Burg Algorithm\n",
    "                feature_vector.extend(self.burg_features(x))\n",
    "\n",
    "            features.append(feature_vector)\n",
    "        return np.array(features)\n",
    "\n",
    "    def hjorth(self, x):\n",
    "        hjorth_activity = np.var(x)\n",
    "        hjorth_mobility = np.sqrt(np.var(np.diff(x)) / hjorth_activity)\n",
    "        hjorth_complexity = np.sqrt(np.var(np.diff(np.diff(x))) / np.var(np.diff(x))) / hjorth_mobility\n",
    "        return [hjorth_activity, hjorth_mobility, hjorth_complexity]\n",
    "\n",
    "    def fft_features(self, x):\n",
    "        fft_vals = np.fft.fft(x)\n",
    "        return [np.max(np.abs(fft_vals))]\n",
    "\n",
    "    def burg_features(self, x):\n",
    "        ar_model = arburg(x, 4)\n",
    "        return ar_model\n",
    "\n",
    "    def process_data(self):\n",
    "        for sub in self.subjects:\n",
    "            file_name = f'A{sub:02d}E.gdf'\n",
    "            file_path = os.path.join(self.dataset_path, file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                raw = self.load_gdf_data(file_path)\n",
    "                features = self.extract_features(raw)\n",
    "                np.save(os.path.join(self.save_path, f'{file_name}_features.npy'), features)\n",
    "                print(f'Features extracted and saved for {file_name}')\n",
    "            else:\n",
    "                print(f'File {file_name} not found in {self.dataset_path}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Example configuration\n",
    "    extractor = FeatureExtractor(\n",
    "        dataset_path='data/BCICIV_2a_gdf',\n",
    "        save_path='output/path',\n",
    "        n_sub=9,\n",
    "        sub_list=list(range(1, 10)),\n",
    "        sample_freq=250,\n",
    "        data_aug=False,\n",
    "        n_hop=0.1,\n",
    "        window_sz=2,\n",
    "        low_freq=0.5,\n",
    "        high_freq=35,\n",
    "        wavelet=True,\n",
    "        f_bank=False,\n",
    "        wpd_noc=False,\n",
    "        n_bands=8,\n",
    "        low_frequencies=np.arange(4, 37, 1),\n",
    "        high_frequencies=np.arange(8, 41, 1),\n",
    "        feature_list=[0, 1, 2, 6, 8, 12, 13, 19, 21]\n",
    "    )\n",
    "    extractor.process_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e6377c-4afd-4359-8867-7a2d690f2387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in directory: ['processed_epoch_sub1_QC']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = 'data/BCICIV_2a_gdf'\n",
    "files = os.listdir(dataset_path)\n",
    "print(\"Files in directory:\", files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b41bb1fe-a696-4c10-8480-c37fa38072fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/BCICIV_2a_gdf/A01E.gdf not found.\n",
      "File data/BCICIV_2a_gdf/A02E.gdf not found.\n",
      "File data/BCICIV_2a_gdf/A03E.gdf not found.\n",
      "File data/BCICIV_2a_gdf/A04E.gdf not found.\n",
      "File data/BCICIV_2a_gdf/A05E.gdf not found.\n",
      "File data/BCICIV_2a_gdf/A06E.gdf not found.\n",
      "File data/BCICIV_2a_gdf/A07E.gdf not found.\n",
      "File data/BCICIV_2a_gdf/A08E.gdf not found.\n",
      "File data/BCICIV_2a_gdf/A09E.gdf not found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "import pywt\n",
    "from scipy import signal\n",
    "from spectrum import arburg\n",
    "import time\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, **kwargs):\n",
    "        # Default dataset path and configuration\n",
    "        self.dataset_path = kwargs.pop('dataset_path', 'data/BCICIV_2a_gdf')\n",
    "        self.save_path = kwargs.pop('save_path', 'output/path')\n",
    "        self.n_sub = kwargs.pop('n_sub', 9)\n",
    "        self.subjects = kwargs.pop('sub_list', list(range(1, self.n_sub + 1)))  # assuming subjects are 1-based\n",
    "        self.sample_freq = kwargs.pop('sample_freq', 250)\n",
    "        self.data_aug = kwargs.pop('data_aug', True)\n",
    "        self.n_hop = kwargs.pop('n_hop', 0.1)\n",
    "        self.window_sz = kwargs.pop('window_sz', 2)\n",
    "        self.low_freq = kwargs.pop('low_freq', 0.5)\n",
    "        self.high_freq = kwargs.pop('high_freq', 35)\n",
    "        self.wavelet = kwargs.pop('wavelet', True)\n",
    "        self.f_bank = kwargs.pop('f_bank', False)\n",
    "        self.wpd_noc = kwargs.pop('wpd_noc', False)\n",
    "        self.n_bands = kwargs.pop('n_bands', 8)\n",
    "        self.low_frequencies = kwargs.pop('low_frequencies', np.arange(4, 37, 1))\n",
    "        self.high_frequencies = kwargs.pop('high_frequencies', np.arange(8, 41, 1))\n",
    "        self.feature_list = kwargs.pop('feature_list', [0, 1, 2, 6, 8, 12, 13, 19, 21])\n",
    "\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "    def downsample(self, x_data, sample_freq=128):\n",
    "        q = self.sample_freq / sample_freq\n",
    "        return mne.filter.resample(x_data, down=q, npad='auto')\n",
    "\n",
    "    def sliding_window(self, x_data, y_data, fs):\n",
    "        duration = x_data.shape[2] / fs\n",
    "        windows = np.arange(0, duration - self.window_sz + self.n_hop, self.n_hop)\n",
    "        n_windows = len(windows)\n",
    "        X_aug = np.zeros((x_data.shape[0] * n_windows, x_data.shape[1], int(self.window_sz * fs)))\n",
    "        y_aug = np.zeros((y_data.shape[0] * n_windows, 1))\n",
    "        for i in range(x_data.shape[0]):\n",
    "            for j in range(x_data.shape[1]):\n",
    "                for idx, w in enumerate(windows):\n",
    "                    X_aug[(i * n_windows) + idx, j, :] = x_data[i, j, int(w * fs):int((w + self.window_sz) * fs)]\n",
    "                    y_aug[(i * n_windows) + idx] = y_data[i]\n",
    "        return X_aug, y_aug\n",
    "\n",
    "    def filter_data(self, x_data, fs, low, high):\n",
    "        iir_params = dict(order=6, ftype='butter')\n",
    "        filt = mne.filter.create_filter(x_data, fs, l_freq=low, h_freq=high, method='iir', iir_params=iir_params, verbose=False)\n",
    "        return signal.sosfiltfilt(filt['sos'], x_data)\n",
    "\n",
    "    def wpd(self, x_data):\n",
    "        coeffs = pywt.WaveletPacket(x_data, 'db4', mode='symmetric', maxlevel=6)\n",
    "        return coeffs\n",
    "\n",
    "    def feature_bands(self, x_data, level=6, start=1, n_bands=8):\n",
    "        all_bands = []\n",
    "        for i in range(x_data.shape[0]):\n",
    "            bands = []\n",
    "            for j in range(x_data.shape[1]):\n",
    "                subbands = []\n",
    "                C = self.wpd(x_data[i, j, :])\n",
    "                wpd_bands = C.get_level(level, 'natural')\n",
    "                for b in range(start, start + n_bands):\n",
    "                    subbands.append(wpd_bands[b].data)\n",
    "                bands.append(subbands)\n",
    "            all_bands.append(bands)\n",
    "        return np.array(all_bands)\n",
    "\n",
    "    def filter_bank(self, x_data, fs):\n",
    "        filtered_X = np.zeros((x_data.shape[0], x_data.shape[1], len(self.low_frequencies), x_data.shape[2]))\n",
    "        for i in range(len(self.low_frequencies)):\n",
    "            filtered_X[:, :, i] = self.filter_data(x_data, fs, self.low_frequencies[i], self.high_frequencies[i])\n",
    "        return filtered_X\n",
    "\n",
    "    def hjorth(self, xV):\n",
    "        hjorth_activity = np.var(xV, axis=1)\n",
    "        hjorth_mobility = np.sqrt(np.var(np.diff(xV, axis=1), axis=1) / hjorth_activity)\n",
    "        hjorth_diffmobility = np.sqrt(np.var(np.diff(np.diff(xV, axis=1), axis=1), axis=1) / np.var(np.diff(xV, axis=1), axis=1))\n",
    "        hjorth_complexity = hjorth_diffmobility / hjorth_mobility\n",
    "        return hjorth_activity, hjorth_mobility, hjorth_complexity\n",
    "\n",
    "    def extract_features(self):\n",
    "        start_time = time.time()\n",
    "        for sub_id in self.subjects:\n",
    "            sub_id_str = f'A{sub_id:02d}E'\n",
    "            path = os.path.join(self.dataset_path, f'{sub_id_str}.gdf')\n",
    "            save_path = os.path.join(self.save_path, f'sub_{sub_id_str}')\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "\n",
    "            # Check if file exists\n",
    "            if not os.path.isfile(path):\n",
    "                print(f\"File {path} not found.\")\n",
    "                continue\n",
    "\n",
    "            # Load GDF file\n",
    "            raw = mne.io.read_raw_gdf(path, preload=True)\n",
    "            x_data = raw.get_data()\n",
    "            y_data = raw.annotations.description  # Assuming annotations are used as labels\n",
    "\n",
    "            if self.sample_freq:\n",
    "                x_data = self.downsample(x_data, sample_freq=self.sample_freq)\n",
    "\n",
    "            if self.data_aug:\n",
    "                x_data, y_data = self.sliding_window(x_data, y_data, fs=self.sample_freq)\n",
    "\n",
    "            x_data = self.filter_data(x_data, fs=self.sample_freq, low=self.low_freq, high=self.high_freq)\n",
    "\n",
    "            if self.wavelet:\n",
    "                x_data = self.feature_bands(x_data, level=6, start=1, n_bands=self.n_bands)\n",
    "                x_data = np.transpose(x_data, (0, 1, 2, 3))\n",
    "            else:\n",
    "                if self.f_bank:\n",
    "                    x_data = self.filter_bank(x_data, fs=self.sample_freq)\n",
    "                x_data = np.transpose(x_data, (0, 1, 2, 3))\n",
    "\n",
    "            features = np.zeros((x_data.shape[0], x_data.shape[1], 22))\n",
    "            for i in range(x_data.shape[0]):\n",
    "                for j in range(x_data.shape[1]):\n",
    "                    x = x_data[i, j, :]\n",
    "                    if 0 in self.feature_list:\n",
    "                        features[i, j, 0] = np.mean(np.abs(x))\n",
    "                    if 1 in self.feature_list:\n",
    "                        features[i, j, 1] = np.sqrt(np.mean(x ** 2))\n",
    "                    if 2 in self.feature_list:\n",
    "                        features[i, j, 2] = np.mean(np.diff(x))\n",
    "                    if 6 in self.feature_list:\n",
    "                        features[i, j, 6] = np.mean(np.abs(x) ** 2)\n",
    "                    if 8 in self.feature_list:\n",
    "                        features[i, j, 8] = np.std(x)\n",
    "                    if 12 in self.feature_list:\n",
    "                        features[i, j, 12] = np.sum(x ** 2)\n",
    "                    if 13 in self.feature_list:\n",
    "                        hjorth_activity, hjorth_mobility, hjorth_complexity = self.hjorth(x)\n",
    "                        features[i, j, 13] = np.mean(hjorth_activity)\n",
    "                        features[i, j, 14] = np.mean(hjorth_mobility)\n",
    "                        features[i, j, 15] = np.mean(hjorth_complexity)\n",
    "                    if 19 in self.feature_list:\n",
    "                        fft_vals = np.abs(np.fft.fft(x))\n",
    "                        features[i, j, 19] = np.max(fft_vals)\n",
    "                    if 21 in self.feature_list:\n",
    "                        ar_coeffs = arburg(x, 4)\n",
    "                        features[i, j, 21] = np.mean(np.abs(ar_coeffs))\n",
    "\n",
    "            features = np.mean(features, axis=1)\n",
    "            features_save_path = os.path.join(save_path, f'features_sub_{sub_id_str}.npy')\n",
    "            np.save(features_save_path, features)\n",
    "            print(f\"Extracted features for subject {sub_id_str} in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    extractor = FeatureExtractor(\n",
    "        dataset_path='data/BCICIV_2a_gdf',\n",
    "        save_path='data/BCICIV_2a_gdf/processed',\n",
    "        n_sub=9\n",
    "    )\n",
    "    extractor.extract_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50340a57-5423-4275-850b-5a8e251d1c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found in directory:\n",
      "processed_epoch_sub1_QC\n",
      "processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = 'data/BCICIV_2a_gdf'\n",
    "\n",
    "# List all files in the directory\n",
    "try:\n",
    "    files = os.listdir(dataset_path)\n",
    "    if not files:\n",
    "        print(f\"No files found in directory: {dataset_path}\")\n",
    "    else:\n",
    "        print(\"Files found in directory:\")\n",
    "        for file in files:\n",
    "            print(file)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Directory not found: {dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "defc4e28-ca3e-4c74-9bcb-ebbd104a145b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking class folder: data/BCICIV_2a_gdf/sub1_segment_QC/class_1\n",
      "Class folder not found: data/BCICIV_2a_gdf/sub1_segment_QC/class_1\n",
      "Checking class folder: data/BCICIV_2a_gdf/sub1_segment_QC/class_2\n",
      "Class folder not found: data/BCICIV_2a_gdf/sub1_segment_QC/class_2\n",
      "Checking class folder: data/BCICIV_2a_gdf/sub1_segment_QC/class_3\n",
      "Class folder not found: data/BCICIV_2a_gdf/sub1_segment_QC/class_3\n",
      "Checking class folder: data/BCICIV_2a_gdf/sub1_segment_QC/class_4\n",
      "Class folder not found: data/BCICIV_2a_gdf/sub1_segment_QC/class_4\n",
      "Data shape: (0,)\n",
      "Labels shape: (0,)\n",
      "No data loaded. Check the file paths and try again.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pywt\n",
    "from scipy import signal\n",
    "from sklearn import preprocessing\n",
    "from spectrum import arburg\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, **kwargs):\n",
    "        # Default dataset path and configuration\n",
    "        self.dataset_path = kwargs.pop('dataset_path', 'data/BCICIV_2a_gdf/sub1_segment_QC')\n",
    "        self.save_path = kwargs.pop('save_path', 'output/path')\n",
    "        self.sample_freq = kwargs.pop('sample_freq', 250)\n",
    "        self.data_aug = kwargs.pop('data_aug', True)\n",
    "        self.n_hop = kwargs.pop('n_hop', 0.1)\n",
    "        self.window_sz = kwargs.pop('window_sz', 2)\n",
    "        self.low_freq = kwargs.pop('low_freq', 0.5)\n",
    "        self.high_freq = kwargs.pop('high_freq', 35)\n",
    "        self.wavelet = kwargs.pop('wavelet', True)\n",
    "        self.f_bank = kwargs.pop('f_bank', False)\n",
    "        self.n_bands = kwargs.pop('n_bands', 8)\n",
    "        self.low_frequencies = kwargs.pop('low_frequencies', np.arange(4, 37, 1))\n",
    "        self.high_frequencies = kwargs.pop('high_frequencies', np.arange(8, 41, 1))\n",
    "        self.feature_list = kwargs.pop('feature_list', [0, 1, 2, 6, 8, 12, 13, 19, 21])\n",
    "\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "    def load_data(self):\n",
    "        # Initialize lists to store data and labels\n",
    "        data_list = []\n",
    "        labels = []\n",
    "\n",
    "        # Loop through class folders\n",
    "        for class_folder in range(1, 5):  # assuming class folders are named class_1 to class_4\n",
    "            class_path = os.path.join(self.dataset_path, f'class_{class_folder}')\n",
    "            print(f\"Checking class folder: {class_path}\")\n",
    "            \n",
    "            if not os.path.isdir(class_path):\n",
    "                print(f\"Class folder not found: {class_path}\")\n",
    "                continue\n",
    "\n",
    "            # Loop through .npy files in each class folder\n",
    "            for file_name in os.listdir(class_path):\n",
    "                if file_name.endswith('.npy'):\n",
    "                    file_path = os.path.join(class_path, file_name)\n",
    "                    print(f\"Loading file: {file_path}\")\n",
    "                    if not os.path.isfile(file_path):\n",
    "                        print(f\"File not found: {file_path}\")\n",
    "                        continue\n",
    "                    data = np.load(file_path)\n",
    "                    print(f\"Loaded data shape: {data.shape}\")\n",
    "                    data_list.append(data)\n",
    "                    labels.extend([class_folder] * len(data))\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        data_array = np.concatenate(data_list, axis=0) if data_list else np.array([])\n",
    "        labels_array = np.array(labels) if labels else np.array([])\n",
    "\n",
    "        print(f\"Data shape: {data_array.shape}\")\n",
    "        print(f\"Labels shape: {labels_array.shape}\")\n",
    "\n",
    "        return data_array, labels_array\n",
    "\n",
    "    def downsample(self, x_data, sample_freq=128):\n",
    "        q = self.sample_freq / sample_freq\n",
    "        return mne.filter.resample(x_data, down=q, npad='auto')\n",
    "\n",
    "    def sliding_window(self, x_data, y_data, fs):\n",
    "        duration = x_data.shape[2] / fs\n",
    "        windows = np.arange(0, duration - self.window_sz + self.n_hop, self.n_hop)\n",
    "        n_windows = len(windows)\n",
    "        X_aug = np.zeros((x_data.shape[0] * n_windows, x_data.shape[1], int(self.window_sz * fs)))\n",
    "        y_aug = np.zeros((y_data.shape[0] * n_windows, 1))\n",
    "        for i in range(x_data.shape[0]):\n",
    "            for j in range(x_data.shape[1]):\n",
    "                for idx, w in enumerate(windows):\n",
    "                    X_aug[(i * n_windows) + idx, j, :] = x_data[i, j, int(w * fs):int((w + self.window_sz) * fs)]\n",
    "                    y_aug[(i * n_windows) + idx] = y_data[i]\n",
    "        return X_aug, y_aug\n",
    "\n",
    "    def filter_data(self, x_data, fs, low, high):\n",
    "        iir_params = dict(order=6, ftype='butter')\n",
    "        filt = mne.filter.create_filter(x_data, fs, l_freq=low, h_freq=high, method='iir', iir_params=iir_params, verbose=False)\n",
    "        return signal.sosfiltfilt(filt['sos'], x_data)\n",
    "\n",
    "    def wpd(self, x_data):\n",
    "        coeffs = pywt.WaveletPacket(x_data, 'db4', mode='symmetric', maxlevel=6)\n",
    "        return coeffs\n",
    "\n",
    "    def feature_bands(self, x_data, level=6, start=1, n_bands=8):\n",
    "        all_bands = []\n",
    "        for i in range(x_data.shape[0]):\n",
    "            bands = []\n",
    "            for j in range(x_data.shape[1]):\n",
    "                subbands = []\n",
    "                C = self.wpd(x_data[i, j, :])\n",
    "                wpd_bands = C.get_level(level, 'natural')\n",
    "                for b in range(start, start + n_bands):\n",
    "                    subbands.append(wpd_bands[b].data)\n",
    "                bands.append(subbands)\n",
    "            all_bands.append(bands)\n",
    "        return np.array(all_bands)\n",
    "\n",
    "    def filter_bank(self, x_data, fs):\n",
    "        filtered_X = np.zeros((x_data.shape[0], x_data.shape[1], len(self.low_frequencies), x_data.shape[2]))\n",
    "        for i in range(len(self.low_frequencies)):\n",
    "            filtered_X[:, :, i] = self.filter_data(x_data, fs, self.low_frequencies[i], self.high_frequencies[i])\n",
    "        return filtered_X\n",
    "\n",
    "    def hjorth(self, xV):\n",
    "        hjorth_activity = np.var(xV, axis=1)\n",
    "        hjorth_mobility = np.sqrt(np.var(np.diff(xV, axis=1), axis=1) / hjorth_activity)\n",
    "        hjorth_diffmobility = np.sqrt(np.var(np.diff(np.diff(xV, axis=1), axis=1), axis=1) / np.var(np.diff(xV, axis=1), axis=1))\n",
    "        hjorth_complexity = hjorth_diffmobility / hjorth_mobility\n",
    "        return hjorth_activity, hjorth_mobility, hjorth_complexity\n",
    "\n",
    "    def extract_features(self):\n",
    "        x_data, y_data = self.load_data()\n",
    "        \n",
    "        if x_data.size == 0:\n",
    "            print(\"No data loaded. Check the file paths and try again.\")\n",
    "            return\n",
    "\n",
    "        if self.sample_freq:\n",
    "            x_data = self.downsample(x_data, sample_freq=self.sample_freq)\n",
    "\n",
    "        if self.data_aug:\n",
    "            x_data, y_data = self.sliding_window(x_data, y_data, fs=self.sample_freq)\n",
    "\n",
    "        x_data = self.filter_data(x_data, fs=self.sample_freq, low=self.low_freq, high=self.high_freq)\n",
    "\n",
    "        if self.wavelet:\n",
    "            x_data = self.feature_bands(x_data, level=6, start=1, n_bands=self.n_bands)\n",
    "            x_data = np.transpose(x_data, (0, 1, 2, 3))\n",
    "        else:\n",
    "            if self.f_bank:\n",
    "                x_data = self.filter_bank(x_data, fs=self.sample_freq)\n",
    "            x_data = np.transpose(x_data, (0, 1, 2, 3))\n",
    "\n",
    "        features = np.zeros((x_data.shape[0], x_data.shape[1], 22))\n",
    "        for i in range(x_data.shape[0]):\n",
    "            for j in range(x_data.shape[1]):\n",
    "                x = x_data[i, j, :]\n",
    "                if 0 in self.feature_list:\n",
    "                    features[i, j, 0] = np.mean(np.abs(x))\n",
    "                if 1 in self.feature_list:\n",
    "                    features[i, j, 1] = np.sqrt(np.mean(x ** 2))\n",
    "                if 2 in self.feature_list:\n",
    "                    features[i, j, 2] = np.mean(np.diff(x))\n",
    "                if 6 in self.feature_list:\n",
    "                    features[i, j, 6] = np.mean(np.abs(x) ** 2)\n",
    "                if 8 in self.feature_list:\n",
    "                    features[i, j, 8] = np.std(x)\n",
    "                if 12 in self.feature_list:\n",
    "                    features[i, j, 12] = np.sum(x ** 2)\n",
    "                if 13 in self.feature_list:\n",
    "                    hjorth_activity, hjorth_mobility, hjorth_complexity = self.hjorth(x)\n",
    "                    features[i, j, 13] = np.mean(hjorth_activity)\n",
    "                    features[i, j, 14] = np.mean(hjorth_mobility)\n",
    "                    features[i, j, 15] = np.mean(hjorth_complexity)\n",
    "                if 19 in self.feature_list:\n",
    "                    fft_vals = np.abs(np.fft.fft(x))\n",
    "                    features[i, j, 19] = np.max(fft_vals)\n",
    "                if 21 in self.feature_list:\n",
    "                    ar_coeffs = arburg(x, 4)\n",
    "                    features[i, j, 21] = np.mean(np.abs(ar_coeffs))\n",
    "\n",
    "        features = np.mean(features, axis=1)\n",
    "        features_save_path = os.path.join(self.save_path, f'features_subject_1.npy')\n",
    "        np.save(features_save_path, features)\n",
    "        print(f\"Extracted features for subject 1 saved to {features_save_path}\")\n",
    "\n",
    "# Example usage\n",
    "feature_extractor = FeatureExtractor()\n",
    "feature_extractor.extract_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4437b967-de98-4170-b991-5d69ce29e5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class folder not found: data/BCICIV_2a_gdf/sub1_segment_QC/sub1/class_1\n",
      "Class folder not found: data/BCICIV_2a_gdf/sub1_segment_QC/sub1/class_2\n",
      "Class folder not found: data/BCICIV_2a_gdf/sub1_segment_QC/sub1/class_3\n",
      "Class folder not found: data/BCICIV_2a_gdf/sub1_segment_QC/sub1/class_4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mne\n",
    "import pywt\n",
    "from scipy import signal\n",
    "from scipy.io import loadmat\n",
    "from sklearn import preprocessing\n",
    "from spectrum import arburg\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, **kwargs):\n",
    "        # Default configuration\n",
    "        self.dataset_path = kwargs.pop('dataset_path', 'data/BCICIV_2a_gdf/sub1_segment_QC')\n",
    "        self.save_path = kwargs.pop('save_path', 'output/path')\n",
    "        self.n_sub = kwargs.pop('n_sub', 1)  # Assuming just one subject here\n",
    "        self.subjects = kwargs.pop('sub_list', [1])\n",
    "        self.sample_freq = kwargs.pop('sample_freq', 250)\n",
    "        self.data_aug = kwargs.pop('data_aug', True)\n",
    "        self.n_hop = kwargs.pop('n_hop', 0.1)\n",
    "        self.window_sz = kwargs.pop('window_sz', 2)\n",
    "        self.low_freq = kwargs.pop('low_freq', 0.5)\n",
    "        self.high_freq = kwargs.pop('high_freq', 35)\n",
    "        self.wavelet = kwargs.pop('wavelet', True)\n",
    "        self.f_bank = kwargs.pop('f_bank', False)\n",
    "        self.wpd_noc = kwargs.pop('wpd_noc', False)\n",
    "        self.n_bands = kwargs.pop('n_bands', 8)\n",
    "        self.low_frequencies = kwargs.pop('low_frequencies', np.arange(4, 37, 1))\n",
    "        self.high_frequencies = kwargs.pop('high_frequencies', np.arange(8, 41, 1))\n",
    "        self.feature_list = kwargs.pop('feature_list', [0, 1, 2, 6, 8, 12, 13, 19, 21])\n",
    "\n",
    "        if not os.path.exists(self.save_path):\n",
    "            os.makedirs(self.save_path)\n",
    "\n",
    "    def filter_data(self, x_data, fs, low, high):\n",
    "        iir_params = dict(order=6, ftype='butter')\n",
    "        filt = mne.filter.create_filter(x_data, fs, l_freq=low, h_freq=high, method='iir', iir_params=iir_params, verbose=False)\n",
    "        return signal.sosfiltfilt(filt['sos'], x_data)\n",
    "\n",
    "    def feature_bands(self, x_data, level=6, start=1, n_bands=8):\n",
    "        all_bands = []\n",
    "        for i in range(x_data.shape[0]):\n",
    "            bands = []\n",
    "            for j in range(x_data.shape[1]):\n",
    "                subbands = []\n",
    "                C = self.wpd(x_data[i, j, :])\n",
    "                wpd_bands = C.get_level(level, 'natural')\n",
    "                for b in range(start, start + n_bands):\n",
    "                    subbands.append(wpd_bands[b].data)\n",
    "                bands.append(subbands)\n",
    "            all_bands.append(bands)\n",
    "        return np.array(all_bands)\n",
    "\n",
    "    def wpd(self, x_data):\n",
    "        coeffs = pywt.WaveletPacket(x_data, 'db4', mode='symmetric', maxlevel=6)\n",
    "        return coeffs\n",
    "\n",
    "    def hjorth(self, xV):\n",
    "        hjorth_activity = np.var(xV, axis=1)\n",
    "        hjorth_mobility = np.sqrt(np.var(np.diff(xV, axis=1), axis=1) / hjorth_activity)\n",
    "        hjorth_diffmobility = np.sqrt(np.var(np.diff(np.diff(xV, axis=1), axis=1), axis=1) / np.var(np.diff(xV, axis=1), axis=1))\n",
    "        hjorth_complexity = hjorth_diffmobility / hjorth_mobility\n",
    "        return hjorth_activity, hjorth_mobility, hjorth_complexity\n",
    "\n",
    "    def extract_features(self):\n",
    "        for sub_id in self.subjects:\n",
    "            sub_id_str = f'sub{sub_id}'\n",
    "            path = os.path.join(self.dataset_path, sub_id_str)\n",
    "            save_path = os.path.join(self.save_path, sub_id_str)\n",
    "            if not os.path.exists(save_path):\n",
    "                os.makedirs(save_path)\n",
    "\n",
    "            # Check for class folders\n",
    "            for class_label in range(1, 5):\n",
    "                class_path = os.path.join(path, f'class_{class_label}')\n",
    "                if not os.path.exists(class_path):\n",
    "                    print(f\"Class folder not found: {class_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Process each file in the class folder\n",
    "                for file_name in os.listdir(class_path):\n",
    "                    if file_name.endswith('.npy'):\n",
    "                        file_path = os.path.join(class_path, file_name)\n",
    "                        print(f\"Processing file: {file_path}\")\n",
    "                        data = np.load(file_path)\n",
    "                        print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "                        # Process data (assuming 22 channels, 1125 samples)\n",
    "                        x_data = data\n",
    "                        fs = self.sample_freq\n",
    "\n",
    "                        # Filter data if needed\n",
    "                        x_data = self.filter_data(x_data, fs, low=self.low_freq, high=self.high_freq)\n",
    "\n",
    "                        # Extract features\n",
    "                        if self.wavelet:\n",
    "                            x_data = self.feature_bands(x_data, level=6, start=1, n_bands=self.n_bands)\n",
    "                            x_data = np.transpose(x_data, (0, 1, 2, 3))\n",
    "                        else:\n",
    "                            if self.f_bank:\n",
    "                                x_data = self.filter_bank(x_data, fs=self.sample_freq)\n",
    "                            x_data = np.transpose(x_data, (0, 1, 2, 3))\n",
    "\n",
    "                        features = np.zeros((x_data.shape[0], x_data.shape[1], 22))\n",
    "                        for i in range(x_data.shape[0]):\n",
    "                            for j in range(x_data.shape[1]):\n",
    "                                x = x_data[i, j, :]\n",
    "                                if 0 in self.feature_list:\n",
    "                                    features[i, j, 0] = np.mean(np.abs(x))\n",
    "                                if 1 in self.feature_list:\n",
    "                                    features[i, j, 1] = np.sqrt(np.mean(x ** 2))\n",
    "                                if 2 in self.feature_list:\n",
    "                                    features[i, j, 2] = np.mean(np.diff(x))\n",
    "                                if 6 in self.feature_list:\n",
    "                                    features[i, j, 6] = np.mean(np.abs(x) ** 2)\n",
    "                                if 8 in self.feature_list:\n",
    "                                    features[i, j, 8] = np.std(x)\n",
    "                                if 12 in self.feature_list:\n",
    "                                    features[i, j, 12] = np.sum(x ** 2)\n",
    "                                if 13 in self.feature_list:\n",
    "                                    hjorth_activity, hjorth_mobility, hjorth_complexity = self.hjorth(x)\n",
    "                                    features[i, j, 13] = np.mean(hjorth_activity)\n",
    "                                    features[i, j, 14] = np.mean(hjorth_mobility)\n",
    "                                    features[i, j, 15] = np.mean(hjorth_complexity)\n",
    "                                if 19 in self.feature_list:\n",
    "                                    fft_vals = np.abs(np.fft.fft(x))\n",
    "                                    features[i, j, 19] = np.max(fft_vals)\n",
    "                                if 21 in self.feature_list:\n",
    "                                    ar_coeffs = arburg(x, 4)\n",
    "                                    features[i, j, 21] = np.mean(np.abs(ar_coeffs))\n",
    "\n",
    "                        features = np.mean(features, axis=1)\n",
    "                        features_save_path = os.path.join(save_path, f'features_{file_name}')\n",
    "                        np.save(features_save_path, features)\n",
    "                        print(f\"Extracted features for file {file_name}. Saved to {features_save_path}\")\n",
    "\n",
    "# Example usage\n",
    "extractor = FeatureExtractor(\n",
    "    dataset_path='data/BCICIV_2a_gdf/sub1_segment_QC',\n",
    "    save_path='output/path'\n",
    ")\n",
    "extractor.extract_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27064a3-8255-4cec-bc08-7dcb97e643eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist: data/BCICIV_2a_gdf/sub1_segment_QC/class_1/sub_epoch_103.npy\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = 'data/BCICIV_2a_gdf/sub1_segment_QC/class_1/sub_epoch_103.npy'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(file_path):\n",
    "    print(f\"File exists: {file_path}\")\n",
    "else:\n",
    "    print(f\"File does not exist: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f61703e3-ae06-4b71-b070-445fd239a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking folder: data/BCICIV_2a_gdf/sub1_segment_QC/class_1\n",
      "Directory not found: data/BCICIV_2a_gdf/sub1_segment_QC/class_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_path = 'data/BCICIV_2a_gdf/sub1_segment_QC'\n",
    "class_folder = 'class_1'\n",
    "\n",
    "# Full path to the expected directory\n",
    "full_path = os.path.join(base_path, class_folder)\n",
    "\n",
    "print(f\"Checking folder: {full_path}\")\n",
    "\n",
    "if os.path.exists(full_path):\n",
    "    print(f\"Directory exists: {full_path}\")\n",
    "    files = os.listdir(full_path)\n",
    "    print(f\"Files in {full_path}: {files}\")\n",
    "else:\n",
    "    print(f\"Directory not found: {full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a24f0-f69c-41a0-9098-729d4f87c1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
